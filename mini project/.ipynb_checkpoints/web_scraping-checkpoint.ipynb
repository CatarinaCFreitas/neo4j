{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "import time\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import progressbar\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download chromedriver executable from https://chromedriver.chromium.org/home\n",
    "\n",
    "# add chromedriver path\n",
    "import sys\n",
    "path = '/Users/catarina/Projects/bin'\n",
    "sys.path.append(path)\n",
    "\n",
    "\n",
    "# 755 is the default numerical permission for files in usr/bin\n",
    "# chromedriver needs a numerical permission equivalent to or greater than 755\n",
    "import os\n",
    "os.chmod(path,755) #664\n",
    "\n",
    "\n",
    "# # configure webdriver to use browser\n",
    "from selenium import webdriver\n",
    "\n",
    "# driver = webdriver.Firefox(executable_path = path + '/geckodriver')\n",
    "# driver = webdriver.Chrome(executable_path = path + '/chromedriver')\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_content(website):\n",
    "    \n",
    "    driver = webdriver.Firefox(executable_path = path + '/geckodriver')\n",
    "    \n",
    "    driver.get(website)\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # scroll page until the end\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")        \n",
    "        \n",
    "        if new_height == last_height:\n",
    "            break\n",
    "            \n",
    "        last_height = new_height\n",
    "        \n",
    "        \n",
    "    content = driver.page_source\n",
    "    \n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    return soup, driver\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_personal_info(soup):\n",
    "    \n",
    "    info = {}\n",
    "    \n",
    "    has_age = soup.find(\"td\", text=\"Age\")\n",
    "    if has_age:\n",
    "        info['age'] = has_age.find_next_sibling(\"td\").text.strip().split(' ')[0]\n",
    "        \n",
    "        \n",
    "    has_birthday = soup.find(\"td\", text=\"Birthday\")\n",
    "    if has_birthday:\n",
    "        info['birthday'] = has_birthday.find_next_sibling(\"td\").text.strip()\n",
    "        \n",
    "        \n",
    "    has_height = soup.find(\"td\", text=\"Height\")\n",
    "    if has_height:\n",
    "        info['height_cm'] = has_height.find_next_sibling(\"td\").text.strip().split('(')[1].split(' ')[0]\n",
    "        \n",
    "        \n",
    "    has_weight = soup.find(\"td\", text=\"Weight\")\n",
    "    if has_weight:\n",
    "        info['weight_kg'] = has_weight.find_next_sibling(\"td\").text.strip().split('(')[1].split(' ')[0]\n",
    "        \n",
    "        \n",
    "    has_eye_color = soup.find(\"td\", text=\"Eye Color\")\n",
    "    if has_eye_color:\n",
    "        info['eye_color'] = has_eye_color.find_next_sibling(\"td\").text.strip().lower()\n",
    "        \n",
    "    \n",
    "    has_hair_color = soup.find(\"td\", text=\"Hair Color\")\n",
    "    if has_hair_color:\n",
    "        info['hair_color'] = has_hair_color.find_next_sibling(\"td\").text.strip().lower()      \n",
    "        \n",
    "        \n",
    "    has_sign = soup.find(\"td\", text=\"Zodiac Sign\")\n",
    "    if has_sign:\n",
    "        info['sign'] = has_sign.find_next_sibling(\"td\").text.strip().lower()\n",
    "        \n",
    "\n",
    "    has_sexuality = soup.find(\"td\", text=\"Sexuality\")\n",
    "    if has_sexuality:\n",
    "        info['sexuality'] = has_sexuality.find_next_sibling(\"td\").text.strip().lower()\n",
    "        \n",
    "        \n",
    "    has_ethnicity = soup.find(\"td\", text=\"Ethnicity\")\n",
    "    if has_ethnicity:\n",
    "        info['ethnicity'] = has_ethnicity.find_next_sibling(\"td\").text.strip().lower()\n",
    "        \n",
    "        \n",
    "    has_nationality = soup.find(\"td\", text=\"Nationality\")\n",
    "    if has_nationality:\n",
    "        info['nationality'] = has_nationality.find_next_sibling(\"td\").text.strip().lower()\n",
    "        \n",
    "        \n",
    "    has_occupation = soup.find(\"td\", text=\"Occupation\")\n",
    "    if has_occupation:\n",
    "        info['occupation'] = has_occupation.find_next_sibling(\"td\").text.strip().lower()\n",
    "        \n",
    "\n",
    "    has_religion = soup.find(\"td\", text=\"Religion\")\n",
    "    if has_religion:\n",
    "        info['religion'] = has_religion.find_next_sibling(\"td\").text.strip().lower()\n",
    "        \n",
    "    \n",
    "    return info\n",
    "\n",
    "\n",
    "def extract_dating_history(soup):\n",
    "\n",
    "    relationships = []\n",
    "\n",
    "    for match in soup.findAll('td', text = re.compile('\\t\\tRelationship\\n|\\t\\tEncounter\\n|\\t\\tMarried\\n')):\n",
    "       \n",
    "        is_rumour = match.find_next_sibling(\"td\").text.strip() == 'R'\n",
    "    \n",
    "        if is_rumour:\n",
    "            continue\n",
    "        \n",
    "        name = match.find_previous_sibling(\"td\").text.strip()\n",
    "        start = match.find_next_sibling(\"td\").find_next_sibling(\"td\")\n",
    "        end = start.find_next_sibling(\"td\")\n",
    "        duration = end.find_next_sibling(\"td\").text.strip()\n",
    "        match_url = match.find_previous_sibling(\"td\").find(\"a\")['href']\n",
    "\n",
    "        relationships.append({'name': name, \n",
    "                              'type': match.text.strip(),\n",
    "                              'start': start.text.strip(), \n",
    "                              'end': end.text.strip(), \n",
    "                              'duration': duration,\n",
    "                              'url': match_url\n",
    "                             })\n",
    "\n",
    "    return relationships\n",
    "\n",
    "\n",
    "\n",
    "def extract_person_data(url):\n",
    "\n",
    "    soup, driver = get_soup_content(url)\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "    i = extract_personal_info(soup)\n",
    "\n",
    "    r = extract_dating_history(soup)\n",
    "\n",
    "    i['name'] = url_to_name(url)\n",
    "\n",
    "    i['relationships'] = r\n",
    "\n",
    "    d = pd.DataFrame.from_dict(i, orient = 'index').transpose()\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_most_popular_celebrities(driver, n):\n",
    "\n",
    "    # scroll page until listing at least 1000 celebrities\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        first_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        content = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(content)\n",
    "\n",
    "        last_item = soup.findAll('i', attrs={'class':'icon-chart-line'})[-1]\n",
    "\n",
    "        nr_items = int(last_item.find_next_sibling(\"span\").text.replace(',',''))\n",
    "\n",
    "        if (nr_items >= n) or (first_height == last_height):\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "    # extract url for details on each celebrity\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    for item in soup.findAll('li', attrs={'class':'ff-grid-box ff-list'}):\n",
    "        urls.append(item.find('a')['href'])\n",
    "\n",
    "\n",
    "    return urls, nr_items\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def url_to_name(url):\n",
    "    return (' ').join([w.capitalize() for w in (url.split('/')[-1].split('-'))])\n",
    "\n",
    "\n",
    "def name_to_url(name):\n",
    "    return name.split('(')[0].strip().lower().replace(' ','-')    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(urls, df):\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        \n",
    "        final_df = df.copy()\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        final_df = pd.DataFrame(columns = ['age', 'birthday', 'height_cm', 'weight_kg', 'eye_color', \n",
    "                                           'hair_color', 'sign', 'sexuality', 'ethnicity', 'nationality',\n",
    "                                           'occupation', 'religion', 'name', 'relationships'])\n",
    "    \n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=len(urls), \n",
    "                                  widgets=[progressbar.Bar('=', '[', ']'), ' ', \n",
    "                                           progressbar.Percentage()])\n",
    "    \n",
    "    bar.start()\n",
    "    \n",
    "    # get information from all urls\n",
    "    \n",
    "    for idx, url in enumerate(urls):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            df = extract_person_data(url)\n",
    "\n",
    "            if df.loc[0,'name'] not in final_df.name.unique():\n",
    "\n",
    "                final_df = pd.concat([final_df, df] , axis = 0, ignore_index = True)\n",
    "\n",
    "\n",
    "            # extract info from all the relationships of this celebrity if they are not already stored\n",
    "\n",
    "            for relationship in df.relationships[0]:\n",
    "\n",
    "                if relationship['name'] in final_df.name.unique():\n",
    "                    continue\n",
    "\n",
    "                df = extract_person_data(relationship['url'])\n",
    "                final_df = pd.concat([final_df, df] , axis = 0, ignore_index = True)     \n",
    "\n",
    "\n",
    "        except Exception as ex:\n",
    "            \n",
    "            print(ex)\n",
    "            \n",
    "            bar.finish()\n",
    "            \n",
    "            return final_df\n",
    "        \n",
    "        \n",
    "        bar.update(idx + 1)\n",
    "        \n",
    "    \n",
    "    bar.finish()       \n",
    "        \n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrape urls pages of the most popular celebrities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrolling down is not always activated - manually ensure it is and restart webdriver if needed\n",
    "\n",
    "driver = webdriver.Firefox(executable_path = path + '/geckodriver')\n",
    "\n",
    "website = \"https://www.whosdatedwho.com/popular\"\n",
    "\n",
    "driver.get(website)\n",
    "\n",
    "content = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run web scraping code\n",
    "\n",
    "urls, nr_items = get_n_most_popular_celebrities(driver, n = 1000)\n",
    "print(nr_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"popular_urls.csv\",\"w\") as f:\n",
    "    wr = csv.writer(f,delimiter=\"\\n\")\n",
    "    wr.writerow(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrape personal info on each celebrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract information from all celebrities in the urls list and from the people their were involved with\n",
    "\n",
    "data = build_dataset(urls, pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "\n",
    "data.to_csv('relationships.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('relationships.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = data.drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â Data correction\n",
    "\n",
    "data_.loc[data_.name == 'Rafael Cebrian','birthday'] = '15th October, 1989'\n",
    "data_.loc[data_.name == 'Ryan Press','birthday'] = '24th October, 1979'\n",
    "data_.loc[data_.name == 'Jessica Vargas','birthday'] = '23th October, 1995'\n",
    "data_.loc[data_.name == 'Dave Gardner','birthday'] = '17th September, 1976'\n",
    "data_.loc[data_.name == 'Tommy Alastra','birthday'] = '14th February, 1976'\n",
    "data_.loc[data_.name == 'Karolyn Pho','birthday'] = '19th January, 1998'\n",
    "data_.loc[data_.name == 'Bonita','birthday'] = '12th December, 1995'\n",
    "data_.loc[data_.name == 'Kaitlin Najjar','birthday'] = '23rd May, 1995'\n",
    "data_.loc[data_.name == 'Natt Weller','birthday'] = '10th May, 1995'\n",
    "data_.loc[data_.name == 'Kaitlin Najjar','birthday'] = '10th May, 1995'\n",
    "data_.loc[data_.name == 'Sophie Coady','birthday'] = '10th November, 1991'\n",
    "data_.loc[data_.name == 'Victor Turpin','birthday'] = '4th March, 1982'\n",
    "data_.loc[data_.name == 'Jack Street','birthday'] = '25th September, 1988'\n",
    "data_.loc[data_.name == 'Viktoria Alexeeva','birthday'] = '13th April, 1995'\n",
    "data_.loc[data_.name == 'Hayes Hargrove','birthday'] = '20th December, 1979'\n",
    "data_.loc[data_.name == 'Cisco Rosado','birthday'] = '29th June, 1979'\n",
    "data_.loc[data_.name == 'Cherie Thibodeaux','birthday'] = '11th September, 1975'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datetime(date):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        date_components = date.replace(',','').split(' ')\n",
    "\n",
    "        date_components = ([date_components[0].replace('st', '').replace('nd', '').replace('rd', '').replace('th', '')]\n",
    "                           + date_components[1:])\n",
    "\n",
    "        date = \"-\".join(date_components)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    return date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zodiac_sign(day, month): \n",
    "    # checks month and date within the valid range \n",
    "    # of a specified zodiac \n",
    "    if month == 12: \n",
    "        return 'sagittarius' if (day < 22) else 'capricorn'\n",
    "\n",
    "    elif month == 1: \n",
    "        return 'capricorn' if (day < 20) else 'aquarius'\n",
    "\n",
    "    elif month == 2: \n",
    "        return 'aquarius' if (day < 19) else 'pisces'\n",
    "\n",
    "    elif month == 3: \n",
    "        return 'pisces' if (day < 21) else 'aries'\n",
    "\n",
    "    elif month == 4: \n",
    "        return 'aries' if (day < 20) else 'taurus'\n",
    "\n",
    "    elif month == 5: \n",
    "        return 'taurus' if (day < 21) else 'gemini'\n",
    "\n",
    "    elif month == 6: \n",
    "        return 'gemini' if (day < 21) else 'cancer'\n",
    "\n",
    "    elif month == 7: \n",
    "        return 'cancer' if (day < 23) else 'leo'\n",
    "\n",
    "    elif month == 8: \n",
    "        return 'leo' if (day < 23) else 'virgo'\n",
    "\n",
    "    elif month == 9: \n",
    "        return 'virgo' if (day < 23) else 'libra'\n",
    "\n",
    "    elif month == 10: \n",
    "        return 'libra' if (day < 23) else 'scorpio'\n",
    "\n",
    "    elif month == 11: \n",
    "        return 'scorpio' if (day < 22) else 'sagittarius'\n",
    "    \n",
    "    elif day == np.nan or month == np.nan:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "    \n",
    "def calculate_age(birthdate): \n",
    "    \n",
    "    if pd.isnull(birthdate):\n",
    "        return np.nan\n",
    "\n",
    "    year = birthdate.year\n",
    "    month = str(birthdate.month)\n",
    "    day = str(birthdate.day)\n",
    "    \n",
    "    age = 2020 - year + int(pd.to_datetime('2020-'+month+'-'+day) <= pd.to_datetime(date.today()))\n",
    "    \n",
    "    return age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_.copy()\n",
    "data_1['birthday'] = data_1['birthday'].apply(process_datetime)\n",
    "data_1['birthday'] = pd.to_datetime(data_1['birthday'], format = '%d-%B-%Y')\n",
    "\n",
    "data_1['sign'] = data_1.birthday.apply(lambda bd: zodiac_sign(bd.day, bd.month))\n",
    "\n",
    "data_1['age'] = data_1.birthday.apply(calculate_age)\n",
    "\n",
    "data_1['n_relationships'] = proc_data.relationships.apply(lambda l: len(eval(l)))\n",
    "\n",
    "data_1 = data_1.rename(columns = {'birthday': 'date_of_birth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data = data_1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to local database 'Kaggle Movie Database' and upload data\n",
    "# extracted from https://www.kaggle.com/rounakbanik/the-movies-dataset\n",
    "\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"gossip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"CREATE CONSTRAINT UniquePersonNameConstraint ON (p:Person) ASSERT p.name IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check creation of constraints\n",
    "graph.run(\"CALL db.constraints()\").data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Person nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in proc_data.iterrows():\n",
    "    graph.run('''\n",
    "        MERGE (p:Person {name:$name})\n",
    "            ON CREATE SET\n",
    "                  p.name = $name,\n",
    "                  p.dateOfBirth = $date_of_birth,\n",
    "                  p.age = toInteger($age),\n",
    "                  p.heightCm = toFloat($height_cm),\n",
    "                  p.weightKg = toFloat($weight_kg),\n",
    "                  p.eyeColor = $eye_color,\n",
    "                  p.hairColor = $hair_color,\n",
    "                  p.sign = $sign,\n",
    "                  p.sexuality = $sexuality,\n",
    "                  p.ethnicity = $ethnicity,\n",
    "                  p.nationality = $nationality,\n",
    "                  p.occupation = $occupation,\n",
    "                  p.religion = $religion,\n",
    "                  p.n_relationships = toInteger($n_relationships)\n",
    "                  ''', \n",
    "        parameters = {\n",
    "          'name': row['name'],\n",
    "          'date_of_birth': str(row.date_of_birth),\n",
    "          'age': row.age,\n",
    "          'height_cm': row.height_cm,\n",
    "          'weight_kg': row.weight_kg,\n",
    "          'eye_color': row.eye_color,\n",
    "          'hair_color': row.hair_color,\n",
    "          'sign': row.sign,\n",
    "          'sexuality': row.sexuality,\n",
    "          'ethnicity': row.ethnicity,\n",
    "          'nationality': row.nationality,\n",
    "          'occupation': row.occupation,\n",
    "          'religion': row.religion,\n",
    "          'n_relationships': row.n_relationships\n",
    "        })\n",
    "    \n",
    "    \n",
    "# check creation of movie nodes\n",
    "\n",
    "graph.run('match (p:Person) return count(p)').data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in proc_data.iterrows():\n",
    "    graph.run('''\n",
    "        WITH apoc.convert.fromJsonList($relationships) AS relationships\n",
    "        UNWIND relationships AS relationships_map\n",
    "        WITH relationships_map['name'] AS p2_name,\n",
    "             relationships_map['type'] AS type,\n",
    "             relationships_map['start'] AS start,\n",
    "             relationships_map['end'] AS end,\n",
    "             relationships_map['duration'] AS duration\n",
    "\n",
    "        \n",
    "        MATCH (p1:Person {name:$p1_name}), (p2:Person {name:p2_name})\n",
    "        \n",
    "        MERGE (p1)-[r:RELATIONSHIP]-(p2)\n",
    "            ON CREATE SET\n",
    "                r.type = type,\n",
    "                r.start = start,\n",
    "                r.end = end,\n",
    "                r.duration = duration\n",
    "                  ''', \n",
    "              \n",
    "        parameters = {\n",
    "          'p1_name': row['name'],\n",
    "          'relationships': row.relationships\n",
    "        })\n",
    "    \n",
    "    \n",
    "# check creation of movie nodes\n",
    "\n",
    "graph.run('MATCH (:Person)-[r:RELATIONSHIP]-(:Person) RETURN COUNT(r)').data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People that shared more relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "MATCH p = (p1:Person)-[r1:RELATIONSHIP]-(p2:Person)-[r2:RELATIONSHIP]-(p3:Person)\n",
    "WHERE p1<>p3 and p1.name < p3.name //and p1.age <= 35 and p3.age <= 35\n",
    "WITH p1, p3, count(distinct(p)) AS c\n",
    "RETURN p1.name, p3.name, c ORDER BY c DESC LIMIT 10\n",
    "'''\n",
    "\n",
    "graph.run(query).data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People up to 40 years old that shared more relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "MATCH p = (p1:Person)-[r1:RELATIONSHIP]-(p2:Person)-[r2:RELATIONSHIP]-(p3:Person)\n",
    "WHERE p1<>p3 and p1.name < p3.name and p1.age <= 40 and p3.age <= 40\n",
    "WITH p1, p3, count(distinct(p)) AS c\n",
    "RETURN p1.name, p3.name, c ORDER BY c DESC LIMIT 10\n",
    "'''\n",
    "\n",
    "graph.run(query).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize nodes\n",
    "\n",
    "query = '''\n",
    "MATCH (p1:Person {name:$name1})-[r1:RELATIONSHIP]-(p3)-[r2:RELATIONSHIP]-(p2:Person {name:$name2}) \n",
    "return p1, p2, p3, r1, r2\n",
    "'''\n",
    "\n",
    "graph.run(query, parameters = {'name1': 'Rihanna', 'name2': 'Rita Ora'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many squares of data?\n",
    "\n",
    "query = '''\n",
    "MATCH (p1:Person)-[:RELATIONSHIP]-(p2:Person)-[:RELATIONSHIP]-(p3:Person)-[:RELATIONSHIP]-(p4:Person)-[:RELATIONSHIP]-(p1) \n",
    "WHERE p1 <> p3 and p2 <> p4 and p1.name < p3.name and p2.name < p4.name\n",
    "return p1, p2, p3, p4 limit 10\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CALL gds.graph.create(\n",
    "    'myGraph',\n",
    "    'Person',\n",
    "    {\n",
    "        RELATIONSHIP: {\n",
    "            orientation: 'UNDIRECTED'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "'''\n",
    "\n",
    "graph.run(query).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CALL gds.louvain.write.estimate('myGraph', { writeProperty: 'community' })\n",
    "YIELD nodeCount, relationshipCount, bytesMin, bytesMax, requiredMemory\n",
    "'''\n",
    "\n",
    "graph.run(query).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CALL gds.louvain.write('myGraph', { writeProperty: 'community' })\n",
    "YIELD communityCount, modularity, modularities\n",
    "'''\n",
    "\n",
    "graph.run(query).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "CALL gds.triangleCount.stream('myGraph')\n",
    "YIELD nodeId, triangleCount\n",
    "WITH nodeId, triangleCount WHERE triangleCount > 0\n",
    "RETURN gds.util.asNode(nodeId).name AS name, triangleCount\n",
    "ORDER BY triangleCount DESC\n",
    "'''\n",
    "\n",
    "graph.run(query).data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
